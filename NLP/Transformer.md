



## `Transformer`

> 当前自然语言处理与多模态 AI 的主力架构

### 结构组成

- 注意力机制_Attention
    
- 位置编码_PositionalEncoding
    
- 多头注意力_MultiHeadAttention
    
- 自注意力与交叉注意力_SelfAndCrossAttention
    


Reference:
- [How Transformers Work: A Detailed Exploration of Transformer Architecture | DataCamp](https://www.datacamp.com/tutorial/how-transformers-work)
- [AI Research Blog - The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture](https://deeprevision.github.io/posts/001-transformer/)
- My TransformerArchitecture
- 