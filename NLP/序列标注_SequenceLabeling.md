
## ä»€ä¹ˆæ˜¯åºåˆ—æ ‡æ³¨ï¼Ÿ

> ç»™å®šä¸€ä¸ªè¾“å…¥åºåˆ— $\boldsymbol{x} = [x_1, x_2, ..., x_n]$ï¼Œé¢„æµ‹å¯¹åº”çš„æ ‡ç­¾åºåˆ— $\boldsymbol{y} = [y_1, y_2, ..., y_n]$ã€‚

æ¯ä¸ªè¾“å…¥å…ƒç´  $x_i$ éƒ½å¯¹åº”ä¸€ä¸ªè¾“å‡ºæ ‡ç­¾ $y_i$ã€‚


## Some sequence labeling tasks

### 2.1 PartOfSpeech Tagging
Assign syntactic labels (e.g., Noun, Verb, etc.) to each word in a sentence.

ğŸ§  Motivation:
- Improves parsing, MachineTranslation, sentiment analysis, and TTS(text-to-speech).

| è¯æ€§     | è‹±æ–‡ç¼©å†™    | è¯æ€§è‹±æ–‡                                  | ç¤ºä¾‹                        |
| ------ | ------- | ------------------------------------- | ------------------------- |
| åè¯     | NN      | Noun                                  | dog, idea                 |
| åŠ¨è¯     | VB      | Verb                                  | run, is                   |
| å½¢å®¹è¯    | JJ      | Adjective                             | red, happy                |
| å‰¯è¯     | RB      | Adverb                                | quickly, very             |
| ä»‹è¯     | IN      | Preposition                           | in, on, under             |
| å† è¯/é™å®šè¯ | DT      | Determiner                            | the, a, some              |
| ä»£è¯     | PRP     | Pronoun                               | he, she, it               |
| è¿è¯     | CC      | Coordinating Conjunction              | and, but                  |
|        | **VB**  | Verb, base form                       | run, eat, go              |
|        | **VBD** | Verb, past tense                      | ran, ate                  |
|        | **VBG** | Verb, gerund or present participle    | running, eating           |
|        | **VBN** | Verb, past participle                 | run, eaten                |
|        | **VBP** | Verb, non-3rd person singular present | run, eat (ç”¨äº I/you/they)  |
|        | **VBZ** | Verb, 3rd person singular present     | runs, eats (ç”¨äº he/she/it) |

### 2.2 Chinese Word Segmentation
Label characters with B/I/E/S tags indicating word boundaries.

| æ ‡ç­¾  | å«ä¹‰            |
| --- | ------------- |
| B   | å•è¯çš„å¼€å§‹ï¼ˆBeginï¼‰  |
| I   | å•è¯çš„ä¸­é—´ï¼ˆInsideï¼‰ |
| E   | å•è¯çš„ç»“æŸï¼ˆEndï¼‰    |
| S   | å•å­—æˆè¯ï¼ˆSingleï¼‰  |

![[Pasted image 20250422094722.png|375]]
### 2.3 Named Entity Recognition (NER)
Label tokens as Person (PER), Location (LOC), Organization (ORG), etc., using BIOES or BIO tagging schemes.

| æ ‡ç­¾    | è¯´æ˜                             |
| ----- | ------------------------------ |
| B-XXX | å®ä½“èµ·å§‹ä½ç½®ï¼ŒXXX æ˜¯ç±»åˆ«ï¼ˆå¦‚ B-PER è¡¨ç¤ºäººåå¼€å§‹ï¼‰ |
| I-XXX | å®ä½“ä¸­é—´                           |
| E-XXX | å®ä½“ç»“æŸï¼ˆBIOES ä¸“æœ‰ï¼‰                 |
| S-XXX | å•å­—å®ä½“ï¼ˆBIOES ä¸“æœ‰ï¼‰                 |
| O     | éå®ä½“ä½ç½®ï¼ˆOutsideï¼‰                 |

![[Pasted image 20250422094631.png|350]]
### 2.4 Semantic Role Labeling (SRL)
Assign predicate-argument structure roles: ARG0 (agent), ARG1 (patient), PRED (predicate), etc.

| æ ‡ç­¾    | å«ä¹‰ï¼ˆä¾ PropBankï¼‰ |
| ----- | -------------- |
| ARG0  | åŠ¨ä½œçš„æ–½äº‹ï¼ˆAgentï¼‰   |
| ARG1  | åŠ¨ä½œçš„å—äº‹ï¼ˆPatientï¼‰ |
| PRED  | è°“è¯ï¼ˆPredicateï¼‰  |
| ARG2+ | å…¶ä»–å‚ä¸è€…ï¼ˆå—ç›Šè€…ã€åœ°ç‚¹ç­‰ï¼‰ |
![[Pasted image 20250422094709.png|325]]

## ModelAndMethod

### HMM

- å‡è®¾æ ‡ç­¾åºåˆ— $y_1, ..., y_n$ æ„æˆé©¬å°”å¯å¤«é“¾
- å­¦ä¹ è½¬ç§»æ¦‚ç‡ + å‘å°„æ¦‚ç‡
- generative model

![[Pasted image 20250422095632.png|450]]
#### HMM çš„å»ºæ¨¡å‡è®¾

HMM å°†è¾“å…¥åºåˆ—çœ‹ä½œæ˜¯**ç”±éšçŠ¶æ€åºåˆ—ç”Ÿæˆè§‚æµ‹åºåˆ—çš„è¿‡ç¨‹**ã€‚

#### æ¨¡å‹ç»„æˆäº”å…ƒç»„ï¼š

$$
\text{HMM} = (S, O, A, B, \pi)
$$

| ç¬¦å·                        | å«ä¹‰                 |
| ------------------------- | ------------------ |
| $Y = \{s_1, ..., s_K\}$   | çŠ¶æ€é›†åˆï¼ˆå¦‚è¯æ€§æ ‡ç­¾ï¼‰        |
| $X = \{o_1, ..., o_V\}$   | è§‚æµ‹é›†åˆï¼ˆå¦‚å•è¯ã€å­—ï¼‰        |
| $Q = P(y_t \mid y_{t-1})$ | çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µtransition |
| $E = P(x_t \mid y_t)$     | å‘å°„æ¦‚ç‡çŸ©é˜µemission     |
| $\pi = P(y_1)$            | åˆå§‹çŠ¶æ€åˆ†å¸ƒ             |

---
#### Decodingï¼ˆViterbi decoding ç®—æ³•ï¼‰
##### ä»»åŠ¡ç›®æ ‡

ç»™å®šè§‚æµ‹åºåˆ— $\boldsymbol{x} = x_1, x_2, ..., x_n$ï¼Œ**å¯»æ‰¾æœ€å¯èƒ½çš„æ ‡ç­¾åºåˆ—** ${\boldsymbol{y}} = y_1,y_2,...,y_n$ï¼š

$$
\hat{\boldsymbol{y}} = \arg\max_{\boldsymbol{y}} P(\boldsymbol{y} \mid \boldsymbol{x})
$$

æ ¹æ®è´å¶æ–¯å®šç†ï¼š

$$
\arg\max_{\boldsymbol{y}} P(\boldsymbol{y} \mid \boldsymbol{x}) 
= \arg\max_{\boldsymbol{y}} \frac{P(\boldsymbol{x}, \boldsymbol{y})}{P(\boldsymbol{x})}
= \arg\max_{\boldsymbol{y}} P(\boldsymbol{x}, \boldsymbol{y})
$$

---

##### è”åˆæ¦‚ç‡å±•å¼€å…¬å¼


$$
P(\boldsymbol{x}, \boldsymbol{y}) = P(y_1) P(x_1 \mid y_1) \prod_{t=2}^{n} P(y_t \mid y_{t-1}) P(x_t \mid y_t)
$$

---


##### ç›®æ ‡å®šä¹‰

$$
\boldsymbol{y}^* = \arg\max_{y_1 \cdots y_n} P(x_1 \cdots x_n, y_1 \cdots y_n)
$$

å®šä¹‰åŠ¨æ€è§„åˆ’å­é—®é¢˜å‡½æ•°ï¼š

> $\pi(i, y_i)$ è¡¨ç¤ºå‰ $i$ ä¸ªè¯ï¼Œä»¥çŠ¶æ€ $y_i$ ç»“å°¾çš„æ‰€æœ‰çŠ¶æ€è·¯å¾„ä¸­æœ€å¤§æ¦‚ç‡å€¼ã€‚

å³ï¼š

$$
\pi(i, y_i) = \max_{y_1 \cdots y_{i-1}} P(x_1 \cdots x_i, y_1 \cdots y_i)
$$
##### é€’æ¨å…¬å¼æ¨å¯¼

ä»è”åˆæ¦‚ç‡çš„å®šä¹‰å‡ºå‘ï¼š

$$
\begin{aligned}
\pi(i, y_i)
&= \max_{y_1 \cdots y_{i-1}} P(x_1 \cdots x_i, y_1 \cdots y_i) \\
&= \max_{y_1 \cdots y_{i-1}} P(x_i \mid y_i) \cdot P(y_i \mid y_{i-1}) \cdot P(x_1 \cdots x_{i-1}, y_1 \cdots y_{i-1}) \\
&= e(x_i \mid y_i) \cdot \max_{y_{i-1}} \left[ q(y_i \mid y_{i-1}) \cdot \pi(i-1, y_{i-1}) \right]
\end{aligned}
$$

å…¶ä¸­ï¼š

- $e(x_i \mid y_i)$ æ˜¯å‘å°„æ¦‚ç‡ï¼›æ˜¯åœ¨éšçŠ¶æ€ $y_i$ ç»™å®šçš„æ¡ä»¶ä¸‹ï¼Œç”Ÿæˆè§‚æµ‹å€¼ $x_i$ çš„æ¦‚ç‡ã€‚ä¹Ÿå°±æ˜¯ï¼š**â€œçŠ¶æ€ $y_i$ å‘å°„å‡ºè§‚æµ‹ $x_i$ çš„æ¦‚ç‡â€**ã€‚
- $q(y_i \mid y_{i-1})$ æ˜¯çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼›
- $\pi(i-1, y_{i-1})$ æ˜¯å‰ä¸€çŠ¶æ€è·¯å¾„çš„æœ€ä¼˜å€¼ã€‚
- $e$,$q$ çš„å€¼å’Œ$i$ ä»¥åŠå¯¹åº”çš„$y_i$ å–å€¼éƒ½æœ‰å…³

##### âœ… åˆå§‹æ¡ä»¶ï¼š

è®¾å®šèµ·å§‹çŠ¶æ€ STARTï¼š

$$
\pi(0, \text{START}) = 1
$$

##### âœ… ç»ˆæ­¢æ¡ä»¶ï¼š

åŠ å…¥ STOP çŠ¶æ€ï¼Œè¡¨ç¤ºç»“æŸï¼š

$$
\begin{aligned}
P(y^*) 
&= \max_{y_1 \cdots y_n} P(x_1 \cdots x_n, y_1 \cdots y_n, \text{STOP}) \\
&= \max_{y_n} q(\text{STOP} \mid y_n) \cdot \pi(n, y_n) \\
&= \pi(n+1, \text{STOP})
\end{aligned}
$$

---

##### å¤æ‚åº¦åˆ†æ

- æ¯ä¸ªæ—¶é—´æ­¥è®¡ç®— $|Y|$ ä¸ªçŠ¶æ€ï¼›
- æ¯ä¸ªçŠ¶æ€è¦ä» $|Y|$ ä¸ªå‰é©±çŠ¶æ€ä¸­å–æœ€å¤§å€¼ï¼›
- æ€»ä½“å¤æ‚åº¦ï¼š$\mathcal{O}(n \cdot |Y|^2)$ï¼Œå…¶ä¸­ $n$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$|Y|$ æ˜¯æ ‡ç­¾æ•°ã€‚

---

##### æ€»ç»“

Viterbi ç®—æ³•é€šè¿‡åŠ¨æ€è§„åˆ’ï¼Œ**é¿å…ç©·ä¸¾æ‰€æœ‰æ ‡ç­¾è·¯å¾„**ï¼Œé«˜æ•ˆåœ°æ‰¾åˆ°æœ€å¤§æ¦‚ç‡è·¯å¾„ï¼ˆæœ€ä¼˜æ ‡ç­¾åºåˆ—ï¼‰ï¼Œæ˜¯ HMM å’Œ CRF ä¸­è§£ç é—®é¢˜çš„æ ‡å‡†è§£æ³•ã€‚


#### çŠ¶æ€è·¯å¾„å›¾ï¼ˆState Trellisï¼‰

![[Pasted image 20250422101932.png|500]]

æ¯åˆ—ä»£è¡¨æ—¶é—´æ­¥ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªå¯èƒ½çŠ¶æ€ï¼Œçº¢è‰²ç®­å¤´è¡¨ç¤ºæœ€ä¼˜è·¯å¾„ã€‚  
è·¯å¾„å¾—åˆ†ä¸ºï¼š

$$
\text{è·¯å¾„å¾—åˆ†} = \prod_{i} e(x_i \mid y_i) \cdot q(y_i \mid y_{i-1})
$$

- **â€œState Trellis: Viterbiâ€** è¡¨ç¤ºç”¨äºâ€œæ‰¾æœ€ä¼˜è·¯å¾„â€çš„ç½‘æ ¼ï¼Œ**æœ€å¤§åŒ–è”åˆæ¦‚ç‡**ï¼›
    
- **â€œState Trellis: Marginalâ€** è¡¨ç¤ºç”¨äºâ€œè¾¹ç¼˜æ¨æ–­â€çš„ç½‘æ ¼ï¼Œ**å¯¹æ‰€æœ‰è·¯å¾„æ±‚å’Œ**ï¼›
---

#### è¾¹ç¼˜æ¨æ–­ï¼šå‰å‘ç®—æ³•ï¼ˆForwardï¼‰
- Dynamic algorithm
ç›®æ ‡ï¼š

$$
P(x_1, ..., x_n) = \sum_{y_1, ..., y_n} P(x_1, ..., x_n, y_1, ..., y_{n)}= \sum_{y_1, ..., y_n}P(y_1) P(x_1 \mid y_1) \prod_{t=2}^{n} P(y_t \mid y_{t-1}) P(x_t \mid y_t)
$$

å¼•å…¥å‰å‘å˜é‡ï¼š

$$
\alpha(i, y_i) = P(x_1, ..., x_i, y_i)
$$

é€’æ¨å…¬å¼ï¼š

$$
\alpha(i, y_i) = e(x_i \mid y_i) \cdot \sum_{y_{i-1}} \alpha(i-1, y_{i-1}) \cdot q(y_i \mid y_{i-1})
$$
![[Pasted image 20250416091140.png|400]]


åˆå§‹ï¼š

$$
\alpha(0, y_0) = 
\begin{cases}
1 & \text{if } y_0 = \text{START} \\
0 & \text{otherwise}
\end{cases}
$$

ç»ˆæ­¢ï¼š

$$
P(x_1 \cdots x_n) = \sum_{y_n} \alpha(n, y_n) \cdot q(\text{STOP} \mid y_n)
$$
![[Pasted image 20250416091246.png|375]]


å¤æ‚åº¦ï¼š$O(n \cdot |Y|^2)$ 

---

#### HMMç›‘ç£è®­ç»ƒï¼ˆSupervised Learningï¼‰

å·²çŸ¥è®­ç»ƒæ•°æ® $\{(x_1, y_1), ..., (x_n, y_n)\}$ï¼Œä½¿ç”¨[æœ€å¤§ä¼¼ç„¶ä¼°è®¡](./../æ•°å­¦åŸºç¡€_MathBasics/æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡_ProbabilityAndStatistics/æ•°ç†ç»Ÿè®¡æ¨æ–­_StatisticalInference#2.2æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰)å­¦ä¹ å‚æ•°ã€‚

è”åˆæ¦‚ç‡å†™ä½œï¼š

$$
P(x, y) = \prod_{i=1}^{n+1} e(x_i \mid y_i) \cdot q(y_i \mid y_{i-1})
$$

MLE ä¼°è®¡å…¬å¼ï¼š

- å‘å°„æ¦‚ç‡ï¼š

  $$
  e(x \mid y) = \frac{c(y, x)}{\sum_{x'} c(y, x')}
  $$

- è½¬ç§»æ¦‚ç‡ï¼š

  $$
  q(y_i \mid y_{i-1}) = \frac{c(y_{i-1}, y_i)}{\sum_{y'} c(y_{i-1}, y')}
  $$

å…¶ä¸­ $c(\cdot)$ ä¸ºè®¡æ•°ï¼ˆå…±ç°æ¬¡æ•°ï¼‰ã€‚


#### HMM Unsupervised Learning

ç»™å®šçš„æ•°æ®å½¢å¼ï¼š

$$
\{x_1, x_2, ..., x_n\}
$$

æˆ‘ä»¬ä¸çŸ¥é“æ¯ä¸ª $x_i$ å¯¹åº”çš„éšè—çŠ¶æ€ï¼ˆæ¯”å¦‚è¯æ€§ã€å®ä½“ï¼‰ï¼Œåªèƒ½è§‚æµ‹åˆ°è¾“å…¥åºåˆ—ã€‚

##### âœ… ç›®æ ‡å‡½æ•°ï¼šæœ€å¤§åŒ–è¾¹ç¼˜ä¼¼ç„¶ï¼ˆMarginal Likelihoodï¼‰

æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ç»„å‚æ•°ï¼Œä½¿å¾—**è§‚æµ‹æ•°æ®å‡ºç°çš„æ¦‚ç‡æœ€å¤§**ï¼š

$$
\max_{\theta} \log P(x_1 \cdots x_n)
$$
$\theta$ ï¼š
1. **åˆå§‹çŠ¶æ€æ¦‚ç‡åˆ†å¸ƒ** $P(y_1)$
2. **çŠ¶æ€è½¬ç§»æ¦‚ç‡åˆ†å¸ƒ** $P(y_t \mid y_{t-1})$
3. **å‘å°„æ¦‚ç‡åˆ†å¸ƒ** $P(x_t \mid y_t)$

ç”±äºéšè—å˜é‡ $y_1 \cdots y_n$ ä¸å¯è§ï¼Œéœ€è¦å¯¹æ‰€æœ‰å¯èƒ½è·¯å¾„æ±‚å’Œï¼š

$$
P(x_1 \cdots x_n) = \sum_{y_1 \cdots y_n} P(x_1 \cdots x_n, y_1 \cdots y_n)
$$

è¿™ä¸ªç›®æ ‡ç§°ä¸ºï¼š**è¾¹ç¼˜ä¼¼ç„¶ï¼ˆMarginal Likelihoodï¼‰**

#####  EM ç®—æ³• for HMM $\to$ Baum-Welch Algorithm
- Can reach a local optimum but not necessarily a global optimum
[EMç®—æ³•](../äººå·¥æ™ºèƒ½å¼•è®º_IntroductionToAI/EMç®—æ³•)

- **E æ­¥**ï¼šè®¡ç®—å½“å‰å‚æ•°ä¸‹ $P(y_t = y \mid x)$ ç­‰åéªŒæ¦‚ç‡ï¼ˆç”¨ forward-backwardï¼‰
- **M æ­¥**ï¼šåˆ©ç”¨æœŸæœ›ç»Ÿè®¡é‡æ–°ä¼°è®¡å‚æ•°ï¼ˆæ›´æ–° $q(y_t \mid y_{t-1})$ï¼Œ$e(x_t \mid y_t)$ï¼‰

##### Forward-Backward
- Forward å‰å‘å˜é‡

å®šä¹‰ï¼š

$$
\alpha(i, y_i) = P(x_1, \dots, x_i,\ y_i)
$$
é€’æ¨ï¼š

$$
\alpha(i, y_i) = \left( \sum_{y_{i-1} \in \mathcal{Y}} \alpha(i - 1, y_{i-1}) \cdot q(y_i \mid y_{i-1}) \right) \cdot e(x_i \mid y_i)
$$

-  Backward åå‘å˜é‡

å®šä¹‰ï¼š

$$
\beta(i, y_i) = P(x_{i+1}, \dots, x_n \mid y_i)
$$
é€’æ¨ï¼š

$$
\beta(i, y_i) = \sum_{y_{i+1} \in \mathcal{Y}} q(y_{i+1} \mid y_i) \cdot e(x_{i+1} \mid y_{i+1}) \cdot \beta(i+1, y_{i+1})
$$
- å•æ—¶åˆ»è¾¹ç¼˜æ¦‚ç‡ï¼ˆæ ‡ç­¾åéªŒï¼‰ï¼š

$$
\gamma(i, y_i) = P(y_i \mid x) = \frac{\alpha(i, y_i) \cdot \beta(i, y_i)}{P(x)}
$$

å…¶ä¸­è§‚æµ‹åºåˆ—çš„æ€»æ¦‚ç‡ï¼š

$$
P(x) = \sum_{y_n \in \mathcal{Y}} \alpha(n, y_n)
$$

- ç›¸é‚»æ ‡ç­¾è¾¹ç¼˜æ¦‚ç‡ï¼ˆåŒæ ‡ç­¾åéªŒï¼‰ï¼š

$$
\xi(i, y_i, y_{i+1}) = P(y_i, y_{i+1} \mid x) = \frac{ \alpha(i, y_i) \cdot q(y_{i+1} \mid y_i) \cdot e(x_{i+1} \mid y_{i+1}) \cdot \beta(i+1, y_{i+1}) }{P(x)}
$$
proof:
$$
P(y_i, y_{i+1} \mid x) = \frac{P(y_i, y_{i+1}, x)}{P(x)}
$$

æŠŠæ•´ä¸ªè§‚æµ‹åºåˆ— $x = x_1, \dots, x_n$ åˆ†æˆä¸‰æ®µï¼š
- å‰ç¼€ $x_1, \dots, x_i$
- å½“å‰è§‚æµ‹ $x_{i+1}$
- åç¼€ $x_{i+2}, \dots, x_n$


$$
P(y_i, y_{i+1}, x) = P(x_1^i, y_i) \cdot P(y_{i+1} \mid y_i) \cdot P(x_{i+1} \mid y_{i+1}) \cdot P(x_{i+2}^n \mid y_{i+1})
$$
- $P(x_1^i, y_i) = \alpha(i, y_i)$
- $P(y_{i+1} \mid y_i) = q(y_{i+1} \mid y_i)$
- $P(x_{i+1} \mid y_{i+1}) = e(x_{i+1} \mid y_{i+1})$
- $P(x_{i+2}^n \mid y_{i+1}) = \beta(i+1, y_{i+1})$




ğŸ” è¿­ä»£è¿‡ç¨‹

**E Step**(Forward-Backward)ï¼š
1. å•æ ‡ç­¾è¾¹ç¼˜æ¦‚ç‡ï¼š
$$
\gamma_t(i) = P(y_t = i \mid x_{1:T})
$$
2. ç›¸é‚»æ ‡ç­¾è¾¹ç¼˜æ¦‚ç‡ï¼š
$$
\xi_t(i,j) = P(y_t = i, y_{t+1} = j \mid x_{1:T})
$$

**M Step**ï¼š
åˆå§‹æ¦‚ç‡ï¼š
$$
\pi(i) = \gamma_1(i)
$$
è½¬ç§»æ¦‚ç‡ï¼š
$$
q_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)}
$$
å‘å°„æ¦‚ç‡ï¼š
$$
e(j,x) = \frac{\sum_{t: x_t = x} \gamma_t(j)}{\sum_{t=1}^{T} \gamma_t(j)}
$$



### MEMM
![[Pasted image 20250425214152.png|325]]


$$
P(y_{1:n} \mid x_{1:n}) = \prod_{t=1}^n P(y_t \mid y_{t-1}, x_t)
$$
$$
P(y_t \mid y_{t-1}, x_t) = \frac{\exp(s(y_{t-1}, y_t, x_t))}{Z(y_{t-1}, x_t)}
$$
(å…¶ä»–å®šä¹‰å’Œä¸‹é¢çš„ç›¸ä¼¼ï¼Œåªæ˜¯æŠŠ$x_t$ æ‰©å±•åˆ°äº†$x_{1:n}$ ,æ‰€ä»¥åªç»™å‡ºä¸‹é¢æ›´generalçš„å®šä¹‰ã€‚)
å…¶ä¸­ï¼š

- $s(y_{t-1}, y_t, x_t) = \boldsymbol{w}^\top f(y_{t-1}, y_t, x_t)$ æ˜¯çº¿æ€§å¾—åˆ†å‡½æ•°ï¼›
- $Z(y_{t-1}, x_t)$ æ˜¯å±€éƒ¨å½’ä¸€åŒ–å› å­


ä¸ºäº†è®©å…¶å‚è€ƒä¸Šä¸‹æ–‡token($x$) ï¼Œæ¨¡å‹å˜ä¸ºï¼š


![[Pasted image 20250425220116.png|350]]

$$
P(y_{1:n} \mid x_{1:n}) = \prod_{t=1}^n P(y_t \mid y_{t-1}, x_{1:n})
$$

$$
P(y_t \mid y_{t-1}, x_{1:n}) = \frac{\exp(s(y_{t-1}, y_t, x_t))}{Z(y_{t-1}, x_{1:n})}
$$
score function:

$$
s(y_{t-1}, y_t, x_{1:n}) = \mathbf{w}^\top f(y_{t-1}, y_t, x_{1:n})
$$

ä¸ºäº†æ»¡è¶³å½’ä¸€åŒ–æ¡ä»¶ï¼š

$$
\sum_{y_t \in \mathcal{Y}} P(y_t \mid y_{t-1}, x_{1:n}) = 1
$$

å¿…é¡»æœ‰ï¼šï¼ˆi.e. def of $Z(\dots)$ï¼‰ 

$$
Z(y_{t-1}, x_{1:n}) = \sum_{y_t \in \mathcal{Y}} \exp(s(y_{t-1}, y_t, x_{1:n}))
$$
å«ä¹‰ï¼š
$Z(y_{t-1}, x_{1:n})$ æ˜¯æ‰€æœ‰å¯èƒ½çš„ $y_t$ ä¸‹ï¼Œæ‰“åˆ†æŒ‡æ•°å’Œçš„å½’ä¸€åŒ–å› å­ã€‚

è¿™æ˜¯ä¸€ä¸ª **å±€éƒ¨ softmax å½’ä¸€åŒ–**ï¼Œä¾èµ–äºï¼š

- å½“å‰è¾“å…¥ $x_{1:n}$ï¼›
- å½“å‰æ ‡ç­¾ $y_t$ï¼›
- å‰ä¸€ä¸ªæ ‡ç­¾ $y_{t-1}$ã€‚

ä½†æ˜¯ä»–å­˜åœ¨ä¸€ä¸ªé—®é¢˜â€”â€”label bias 
>[!note] label bias
> å‡è®¾æˆ‘ä»¬ä¸€å…±æœ‰$Y$ä¸ªlabelï¼Œé‚£ä¹ˆå¯¹äºä¸¤ä¸ªä¸åŒçš„label $i$, $j$ å¦‚æœ$q_{k_1,i},q_{k_2,i}$, but $q_{k_3,j}\dots q_{k_6,j}$ 

æ‰€ä»¥è€ƒè™‘å…¨å±€å½’ä¸€åŒ–å‡½æ•°$\to$ CRF

- MEMM ä½¿ç”¨çš„æ˜¯å±€éƒ¨å½’ä¸€åŒ–ï¼›
- CRF ä½¿ç”¨çš„æ˜¯**å…¨å±€å½’ä¸€åŒ–**ï¼š

$$
Z(x_{1:n}) = \sum_{\hat{y}_{1:n}} \exp\left( \sum_t s(\hat{y}_{t-1}, \hat{y}_t, x_{1:n}) \right)
$$

### CRF

- åˆ¤åˆ«å¼æ¨¡å‹ï¼Œç›´æ¥å»ºæ¨¡ $P(\boldsymbol{y} \mid \boldsymbol{x})$
- å…è®¸ä½¿ç”¨ä¸°å¯Œçš„ç‰¹å¾å‡½æ•°
- é€šå¸¸ä½¿ç”¨ Viterbi ç®—æ³•è§£ç æœ€ä¼˜æ ‡ç­¾åºåˆ—

![[Pasted image 20250425222600.png|400]]




$$
P(y_{1:n} \mid x_{1:n}) = \frac{1}{Z(x_{1:n})} \prod_t \exp(s(y_{t-1}, y_t, x_{1:n}))
$$


$$
Z(x_{1:n}) = \sum_{y'_{1:n}} \prod_t \exp(s(y'_{t-1}, y'_t, x_{1:n}))
$$
#### Learning
##### Objective Function (Supervised Learning)

Given a set of labeled sequences $\{(x^{(i)}_{1:n}, y^{(i)}_{1:n})\}$, the CRF defines the conditional probability:


$$
P(y_{1:n} \mid x_{1:n}) = \frac{1}{Z(x_{1:n})} \prod_t \exp(s(y_{t-1}, y_t, x_{1:n}))
$$


where:
- $s(y_{t-1}, y_t, x_{1:n})$ is the **score function**, typically linear in feature weights.
- $Z(x_{1:n})$ is the **partition function**:


$$
Z(x_{1:n}) = \sum_{y'_{1:n}} \prod_t \exp(s(y'_{t-1}, y'_t, x_{1:n}))
$$

**Training Goal**

Maximize the log-likelihood over training data:

$$
\log P(y_{1:n} \mid x_{1:n}) = \sum_t s(y_{t-1}, y_t, x_{1:n}) - \log Z(x_{1:n})
$$

**Gradient Computation**

The gradient of the log-likelihood involves:

1. **Empirical counts** of features from the ground truth sequence.
2. **Expected counts** of features under the model distribution (requires computing marginal probabilities).

These expected counts are computed using the **Forward-Backward algorithm**, similar to HMMs.

---
**Optimization Methods**

- **Gradient Descent / L-BFGS**: Often used with auto-differentiation frameworks.
- **Structured SVM variant**: Uses a **margin-based loss**:


$$
L_{\text{SSVM}} = \max_{y \neq y^*} [s(y) + \Delta(y, y^*)] - s(y^*)
$$


where $\Delta$ is a task-specific cost (e.g., number of misclassified labels).

**Unsupervised Learning**

CRFs cannot model $P(x)$, making **unsupervised learning** tricky. A workaround is the **CRF Autoencoder (CRF-AE)**:
- CRF acts as the **encoder** to produce latent labels.
- A simple decoder tries to **reconstruct** the input sequence from the labels.
- The training objective maximizes the **reconstruction likelihood**.
![[Pasted image 20250425223511.png|200]]




### NeuralNetworks

#### RNN 
åˆ©ç”¨äº†context of each wordï¼Œä½†æ˜¯æ²¡æœ‰åˆ©ç”¨relations between neighboring labels

#### Bidirectional RNN
åˆ©ç”¨äº†context of each wordï¼Œä½†æ˜¯æ²¡æœ‰åˆ©ç”¨relations between neighboring labels
#### BiLSTM
- ä½¿ç”¨åŒå‘ LSTM å¤„ç†ä¸Šä¸‹æ–‡ä¿¡æ¯
- è¾“å‡ºæ¯ä¸ªä½ç½®çš„è¡¨ç¤ºç”¨äºåˆ†ç±»
åˆ©ç”¨äº†context of each wordï¼Œä½†æ˜¯æ²¡æœ‰åˆ©ç”¨relations between neighboring labels
#### Transformer / BERT
åˆ©ç”¨äº†context of each wordï¼Œä½†æ˜¯æ²¡æœ‰åˆ©ç”¨relations between neighboring labels

#### BiLSTM/Transformer + CRF
åŒæ—¶åˆ©ç”¨äº†context of each wordï¼Œrelations between neighboring labels
- Neural Network è®¡ç®—CRFä¸­çš„scoreï¼ŒCRF å»é¢„æµ‹æ ‡ç­¾
- æ˜¯ç›®å‰ä¸»æµæ–¹æ³•ä¹‹ä¸€

### EvaluationOfModel

| æ¨¡å‹ç±»å‹                               | ä¸Šä¸‹æ–‡æ„ŸçŸ¥ ğŸ˜¢ï¼ˆ1ï¼‰                              | æ ‡ç­¾é—´å»ºæ¨¡ ğŸ™‚ï¼ˆ2ï¼‰             |
| ---------------------------------- | ---------------------------------------- | ----------------------- |
| **HMM** (Hidden Markov Model)      | âŒ ä¸è€ƒè™‘ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆä»…åŸºäºå½“å‰ä½ç½®çš„è§‚æµ‹ï¼‰                   | âœ… æ˜¾å¼å»ºæ¨¡ $P(y_tâˆ£y_{tâˆ’1})$ |
| **CRF** (Conditional Random Field) | âŒï¼ˆçº¿æ€§ CRF ä»…ä½¿ç”¨å½“å‰ä½ç½®ç‰¹å¾ï¼‰  <br>ï¼ˆä½†å¯é€šè¿‡æ‰‹å·¥ç‰¹å¾åŠ å…¥ä¸Šä¸‹æ–‡ï¼‰ | âœ… æ˜¾å¼å»ºæ¨¡æ•´ä¸ªæ ‡ç­¾åºåˆ—çš„æ¡ä»¶æ¦‚ç‡       |
| **ç‹¬ç«‹ç¥ç»ç½‘ç»œ**ï¼ˆå¦‚ BiLSTM + Softmaxï¼‰     | âœ… ç½‘ç»œå¯æ•æ‰ä¸Šä¸‹æ–‡ï¼ˆå¦‚ BiLSTM/Transformerï¼‰         | âŒ æ¯ä¸ªä½ç½®ç‹¬ç«‹é¢„æµ‹ï¼Œä¸å»ºæ¨¡æ ‡ç­¾ä¾èµ–      |
| **ç¥ç»ç½‘ç»œ + CRF**ï¼ˆå¦‚ BiLSTM + CRFï¼‰     | âœ… ä¸Šä¸‹æ–‡ç”±ç½‘ç»œå»ºæ¨¡ï¼ˆå¦‚ BiLSTMï¼‰                     | âœ… æ ‡ç­¾ä¾èµ–ç”± CRF å±‚å»ºæ¨¡         |







---



## LossFunctionsAndTraining

- å¯¹äºåˆ†ç±»æ¨¡å‹ï¼ˆå¦‚ BiLSTMï¼‰ï¼Œå¸¸ç”¨ï¼š
  $$
  \mathcal{L} = - \sum_{i=1}^{n} \log P(y_i \mid h_i)
  $$

- å¯¹äº CRFï¼Œåˆ™ä½¿ç”¨ï¼š
  $$
  \mathcal{L} = -\log P(\boldsymbol{y} \mid \boldsymbol{x}) = -\left( s(\boldsymbol{x}, \boldsymbol{y}) - \log Z(\boldsymbol{x}) \right)
  $$

å…¶ä¸­ $s(\cdot)$ æ˜¯æ‰“åˆ†å‡½æ•°ï¼Œ$Z(\cdot)$ æ˜¯é…åˆ†å‡½æ•°ã€‚



