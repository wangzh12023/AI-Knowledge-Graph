

# 决策树（Decision Tree）

决策树是一种常用的监督学习模型，可以用于**分类**和**回归**任务，其基本思想是对数据进行**特征划分**，最终构造出一个类似流程图的“树形结构”模型。

---

## 一、基本结构

决策树由以下几种节点组成：

- **根节点（Root Node）**：表示整个数据集；
- **内部节点（Internal Node）**：表示某个特征的判断条件；
- **叶子节点（Leaf Node）**：表示最终的类别或预测值。

每个内部节点对应一个特征划分，每个分支对应该特征的一个取值或区间。

---

## 二、决策树分类任务基本流程

1. 选择最优特征作为当前节点的划分标准；
2. 根据该特征的取值划分子集；
3. 对每个子集递归构建子树；
4. 满足停止条件后，生成叶子节点。

---

## 三、特征选择准则（信息度量）

### 1. 信息熵（Entropy）

衡量数据集的不确定性：

$$
H(D) = - \sum_{k=1}^{K} p_k \log_2 p_k
$$

其中 $p_k$ 表示第 $k$ 类样本所占的比例。

---

### 2. 信息增益（ID3 算法使用）

选择特征 $A$ 进行划分所带来的信息熵减少：

$$
\text{Gain}(D, A) = H(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|} H(D^v)
$$

其中 $D^v$ 是在特征 $A$ 上取值为 $v$ 的子集。

---

### 3. 信息增益率（C4.5 算法使用）

修正信息增益偏向于取值较多的特征：

$$
\text{GainRatio}(D, A) = \frac{\text{Gain}(D, A)}{H_A(D)}
$$

其中：

$$
H_A(D) = -\sum_{v=1}^{V} \frac{|D^v|}{|D|} \log_2 \left( \frac{|D^v|}{|D|} \right)
$$

---

### 4. 基尼指数（Gini）（CART 使用）

衡量数据的不纯度：

$$
Gini(D) = 1 - \sum_{k=1}^{K} p_k^2
$$

对于二分类，基尼值越小越纯。

---

## 四、剪枝（Pruning）

剪枝用于防止决策树过拟合：

### 1. 预剪枝（Pre-pruning）

- 限制树的最大深度；
- 限制每个节点最小样本数；
- 限制信息增益小于某阈值时不再划分。

### 2. 后剪枝（Post-pruning）

- 先生成整棵树；
- 然后自底向上判断是否剪除子树，替换为叶子节点（依据损失函数或交叉验证误差）。

---

## 五、回归树（Regression Tree）

决策树也可以用于回归问题，称为[回归树](决策树回归_DecisionTreeRegression)。

## 六、优缺点总结

### ✅ 优点：

- 模型直观，可视化强；
- 可处理非线性数据；
- 特征无需归一化；
- 可处理分类与回归任务；
- 可处理缺失值和类别型变量。

### ❌ 缺点：

- 容易过拟合；
- 对噪声敏感；
- 小变动可能导致结构大变（不稳定）；
- 不擅长建模连续平滑函数。

---

## 七、常见算法

| 算法 | 特征选择准则 | 是否剪枝 | 备注 |
|------|----------------|-----------|------|
| ID3 | 信息增益 | 否 | 只能处理离散特征 |
| C4.5 | 信息增益率 | 是（后剪枝） | 可处理连续特征 |
| CART | 基尼指数 / MSE | 是 | 可用于分类和回归，二叉树结构 |


