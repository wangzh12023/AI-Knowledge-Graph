
## 正则化（Regularization）

正则化是机器学习中用于防止模型过拟合的一种常用技术。其核心思想是在损失函数中加入对模型复杂度的惩罚项，从而抑制过大的模型参数，使模型更具有泛化能力。

---

## 一、正则化的动机

在监督学习中，我们通常最小化经验风险：

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(y^{(i)}, f(x^{(i)}; \theta))
$$

若模型参数过大、过拟合训练集，则对新数据的泛化能力可能很差。**正则化的目标**是：

> 控制模型复杂度，限制参数规模，提升泛化能力。

---

## 二、常见的正则化形式

### 1. L2 正则化（岭回归 / Ridge Regularization）

在损失函数中加入参数向量的 $L_2$ 范数平方：

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(y^{(i)}, f(x^{(i)})) + \frac{\lambda}{2} \|\theta\|_2^2
$$

其中：

$$
\|\theta\|_2^2 = \sum_{j=1}^{n} \theta_j^2
$$

特点：

- 惩罚大参数，使模型更“平滑”；
- 会将参数**压缩为接近 0**，但**不会完全为 0**；
- 解是连续可导的，优化方便。

---

### 2. L1 正则化（Lasso）

加入参数向量的 $L_1$ 范数：

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(y^{(i)}, f(x^{(i)})) + \lambda \|\theta\|_1
$$

其中：

$$
\|\theta\|_1 = \sum_{j=1}^{n} |\theta_j|
$$

特点：

- 具有**稀疏性**，可自动进行特征选择；
- 有的参数会被直接压缩为 0；
- 不连续可导，优化可使用坐标下降或子梯度法。

---

### 3. 弹性网（Elastic Net）

结合 L1 和 L2 的优点：

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(y^{(i)}, f(x^{(i)})) + \lambda_1 \|\theta\|_1 + \frac{\lambda_2}{2} \|\theta\|_2^2
$$

适用于高维稀疏特征且特征高度相关的场景。

---

## 三、几何解释

- **L2 正则化约束区域**是圆形（二维下）；
- **L1 正则化约束区域**是菱形；
- 在与损失函数等高线相切的点，L1 更容易落在坐标轴上 → 导致稀疏性（即 $\theta_j = 0$）；
- 可视化角点与稀疏解之间的关系。

---

## 四、正则化的调参：$\lambda$ 的作用

- $\lambda$ 越大：正则项权重越大，模型越简单，欠拟合风险增大；
- $\lambda$ 越小：正则项作用减弱，模型趋向原始最小损失解，可能过拟合；
- $\lambda$ 通常通过交叉验证（Cross-Validation）选取。

---

## 五、正则化在不同模型中的体现

| 模型 | 无正则化损失 | 正则化形式 |
|------|---------------|------------|
| 线性回归 | MSE | 岭回归 / Lasso |
| 逻辑回归 | Cross-Entropy | L1 / L2 正则项 |
| 神经网络 | Cross-Entropy / MSE | 权重惩罚、Dropout、BatchNorm |
| 支持向量机 | Hinge Loss | $\|\boldsymbol{w}\|^2$ 即 L2 正则项 |

---

## 六、与贝叶斯方法的关系

正则化项对应于先验分布：

- **L2 正则化 ⇔ 高斯先验**：
  $$
  p(\theta) \propto \exp\left(-\frac{1}{2\sigma^2} \|\theta\|_2^2 \right)
  $$

- **L1 正则化 ⇔ 拉普拉斯先验**：
  $$
  p(\theta) \propto \exp(-\lambda |\theta|)
  $$

因此，正则化也可被理解为**最大后验估计（MAP）** 的一部分。

---

## 七、常见问题与注意事项

- 正则项是否应用于偏置项 $b$：通常**不对 $b$ 使用正则**；
- 对于数据标准化很重要：不同尺度特征会影响正则化效果；
- L1 适合用于特征选择，L2 更常用于参数控制。

